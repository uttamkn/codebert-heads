{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4862f4ee-f4dd-46b0-8c6d-c91e6aa08982",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['repository_name', 'func_path_in_repository', 'func_name', 'whole_func_string', 'language', 'func_code_string', 'func_code_tokens', 'func_documentation_string', 'func_documentation_tokens', 'split_name', 'func_code_url'],\n",
      "    num_rows: 23107\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load only validation dataset of python from hf\n",
    "dataset = load_dataset(\"code_search_net\", \"python\", split=\"validation\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee5efbd-80d9-4a6f-a56c-1aed37b10457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset[0]['whole_func_string'])\n",
    "# print(dataset[0]['func_code_string'])\n",
    "# print(dataset[0]['func_documentation_string'])\n",
    "\n",
    "# Convert the dataset into pairs of docstring-code\n",
    "str_pairs = [(d['func_documentation_string'], d['func_code_string']) for d in dataset]\n",
    "\n",
    "# pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c737d55b-c674-44aa-9d88-790ac4660de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4535 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# testion out roberta tokenizer\n",
    "from transformers.models.roberta import RobertaTokenizer\n",
    "\n",
    "tnizer = RobertaTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "x = tnizer(str_pairs[0][1])\n",
    "\n",
    "# This will give us both input_ids and attention mask\n",
    "# print(f\"input: {str_pairs[0][1]}\\ntokenizer output: {x}\") # testing for the code string of first datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56ab7e04-845a-48cc-8e1e-4fe9adf03663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Init model\n",
    "from transformers.models.roberta import RobertaModel\n",
    "\n",
    "# Using roberta because codebert is built on top of it (roberta has the same architecture as bert, the only difference is that it is trained on much more data, and NSP task is skipped while training).\n",
    "model = RobertaModel.from_pretrained(\"microsoft/codebert-base\", output_attentions=True, attn_implementation=\"eager\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3210a57e-34d2-4395-9e2c-51682a38aace",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0, 40249,    10,  1844,  1343,  1421,     4, 50140,  1437,  1437,\n",
       "          1437, 47930, 50118,  1437,  1437,  1437, 48364, 50118,  1437,  1437,\n",
       "          1437, 16028,    35,  6545,     4, 16040,   705, 50118,  1437,  1437,\n",
       "          1437,  1437,  1437,  1437,  1437,  1737,     7,  2341,    15, 50118,\n",
       "          1437,  1437,  1437,  1546,    35,  6755,    50,    10,  5043, 50118,\n",
       "          1437,  1437,  1437,  1437,  1437,  1437,  1437, 26739,  1546,     7,\n",
       "           304,    25,    10,  2231,  5043, 36612,   757,  2630,     4,   318,\n",
       "          6755,     6,    34,     7,    28,    65,     9,     5,  2523,     9,\n",
       "          3382,  3092,    11, 11909, 38630,     4, 27278,     4, 43457, 50118,\n",
       "          1437,  1437,  1437,  1437,  1437,  1437,  1437,    36, 18517,   642,\n",
       "             6,   740, 15688,     6, 15380,  1215,  8338,   322,   318,    10,\n",
       "          5043,     6,   197,   185,    41, 15306,  7281,   368,     8,   671,\n",
       "            10, 42715, 15594,  7281,   368,     6,    61,     2,     2,  9232,\n",
       "          1532,  1640, 41124,     6, 50118,  1437,  1437,  1437,  1437,  1437,\n",
       "          1437,  1437,  1437,  1437,  1546,     6, 50118,  1437,  1437,  1437,\n",
       "          1437,  1437,  1437,  1437,  1437,  1437,  5018,  5214, 29802,     6,\n",
       "         50118,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,\n",
       "           784,   338,  5214,   245,   242,    12,   306,     6, 50118,  1437,\n",
       "          1437,  1437,  1437,  1437,  1437,  1437,  1437,  1437,   746,  1215,\n",
       "         10519,   990, 26378,  5214,  1866,   151,     6, 50118,  1437,  1437,\n",
       "          1437,  1437,  1437,  1437,  1437,  1437,  1437, 21944,  1215, 10799,\n",
       "          5214,   245, 14200,     6, 50118,  1437,  1437,  1437,  1437,  1437,\n",
       "          1437,  1437,  1437,  1437,  6942,  1215,   506, 22870,  5214,   288,\n",
       "             4,   134,     6, 50118,  1437,  1437,  1437,  1437,  1437,  1437,\n",
       "          1437,  1437,  1437,  6942,  1215,  6156,  1215, 26378,  5214,   288,\n",
       "             4,  4197,     6, 50118,  1437,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying out inference for one docstring-code pair\n",
    "doc, code = str_pairs[0]\n",
    "\n",
    "# setting max_length because model expects fixed length inputs\n",
    "inputs = tnizer(doc, code, padding=\"max_length\", truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94a794cb-308c-49d2-a947-5f12263ccd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5.3649e-01, 4.0284e-03, 2.3822e-03,  ..., 4.3576e-03, 1.2357e-03,\n",
      "         4.9639e-03],\n",
      "        [2.7465e-01, 2.4957e-02, 2.0957e-02,  ..., 2.8823e-03, 1.1690e-03,\n",
      "         2.4128e-03],\n",
      "        [2.8285e-02, 1.0770e-01, 9.4607e-02,  ..., 3.7162e-03, 1.5548e-03,\n",
      "         1.4725e-03],\n",
      "        ...,\n",
      "        [1.7232e-03, 2.3840e-04, 7.9932e-05,  ..., 8.3183e-02, 2.1781e-02,\n",
      "         7.5156e-03],\n",
      "        [5.7953e-04, 1.0877e-04, 4.7337e-05,  ..., 2.9940e-01, 5.0779e-02,\n",
      "         1.9509e-02],\n",
      "        [1.3488e-02, 4.8767e-04, 1.3569e-04,  ..., 3.5414e-01, 9.7978e-02,\n",
      "         8.2096e-02]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# This is a tuple of (layer1, layer2, ..., layer12), each shape: [1, num_heads, seq_len, seq_len]\n",
    "# attentions[layer][batch][head] -> 2d array representing attention scores\n",
    "print(outputs.attentions[0][0][0]) # attention scores of first attention head in first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23435769-5c7d-4d7c-a67e-bba638ac0811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
